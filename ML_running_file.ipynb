{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "ed4cbd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   Address  Property Type  Bedrooms  \\\n",
      "0                   2 Dunman Road (439188)              0         5   \n",
      "1                            Shelford Road              1         5   \n",
      "2  60H Kent Ridge Hill Residences (117321)              0         1   \n",
      "3                               Marina Way              0         1   \n",
      "4                           Lentor Central              0         1   \n",
      "\n",
      "   Bathrooms  Asking Price  Size  Age  District  Years_Left  Primary Schools  \\\n",
      "0          3       3764000  1679   54        15        45.0                3   \n",
      "1          4      10000000  5134   41        11      9958.0                2   \n",
      "2          1       1030000   474    1         5        98.0                0   \n",
      "3          1       1630000   700    7         1        92.0                1   \n",
      "4          1       1358000   527   94        26         5.0                3   \n",
      "\n",
      "   Secondary Schools  Shopping Malls  Groceries & Supermarts  \\\n",
      "0                  3               3                       3   \n",
      "1                  3               1                       3   \n",
      "2                  0               3                       3   \n",
      "3                  0               3                       3   \n",
      "4                  3               1                       3   \n",
      "\n",
      "   No. of Amenities  Latitude   Longitude  \n",
      "0                12  1.307933  103.890372  \n",
      "1                 9  1.325062  103.812525  \n",
      "2                 6  1.281103  103.790179  \n",
      "3                 7  1.277143  103.853886  \n",
      "4                10  1.385547  103.834165  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the directory containing your files\n",
    "directory = 'data'\n",
    "\n",
    "# List all CSV files in the directory\n",
    "csv_files = [f for f in os.listdir(directory) if f.startswith('geocoded_addresses') and f.endswith('.csv')]\n",
    "\n",
    "# Sort the files by name to maintain the order (optional, depending on your needs)\n",
    "csv_files.sort()\n",
    "\n",
    "# Create an empty list to hold DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Loop through the files and read each one into a DataFrame\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(directory, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Concatenate all the DataFrames in the list into a single DataFrame\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Now you can use combined_df as your single DataFrame containing all data\n",
    "print(combined_df.head())  # to display the first few rows of the combined DataFrame\n",
    "combined_df = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "20d75468",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['Latitude', 'Longitude','Years_Left'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "78389417",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "# Drop the 'Address' column\n",
    "df = df.drop(columns=['Address'])\n",
    "\n",
    "# Ensure 'Years_Left' is of type float\n",
    "df['Years_Left'] = df['Years_Left'].astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "628b3828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode 'District'\n",
    "df = pd.get_dummies(df, columns=['District'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "3d67a06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geohash2\n",
    "\n",
    "# Define a function to apply Geohash encoding\n",
    "def encode_geohash(df, lat_col='Latitude', lon_col='Longitude', precision=5):\n",
    "    \"\"\" Encodes latitude and longitude into geohash. \"\"\"\n",
    "    df['geohash'] = df.apply(lambda x: geohash2.encode(x[lat_col], x[lon_col], precision=precision), axis=1)\n",
    "    return df\n",
    "\n",
    "# Apply the function to your dataframe\n",
    "df = encode_geohash(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "135b03c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_encode_geohash(df, target_col):\n",
    "    \"\"\" Target encodes the geohash to find average values of the target variable per geohash. \"\"\"\n",
    "    # Calculate mean target per geohash\n",
    "    geohash_target_mean = df.groupby('geohash')[target_col].mean().reset_index(name='geohash_target_mean')\n",
    "    \n",
    "    # Merge this back on the original dataframe\n",
    "    df = df.merge(geohash_target_mean, how='left', on='geohash')\n",
    "    return df\n",
    "\n",
    "# Assuming your target column is named appropriately\n",
    "df = target_encode_geohash(df, target_col='Asking Price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "b06b5d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming df is your DataFrame and selecting numeric columns for scaling\n",
    "numeric_cols = ['Bedrooms', 'Bathrooms', 'Size', 'Age', 'Years_Left', \n",
    "                'Primary Schools', 'Secondary Schools', 'Shopping Malls', 'Groceries & Supermarts', \n",
    "                'No. of Amenities','geohash_target_mean']\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19d08b9",
   "metadata": {},
   "source": [
    "# Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "dcdb7d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assume 'df' is your DataFrame and it's ready for splitting\n",
    "# 'X' contains all features except the target variable 'Asking Price'\n",
    "# 'y' is the target variable 'Asking Price'# Exclude the 'geohash' column from the feature set\n",
    "X = df.drop(['Asking Price', 'geohash','Latitude', 'Longitude', 'Primary Schools', 'Secondary Schools', 'Shopping Malls', 'Groceries & Supermarts',], axis=1)  # Assuming 'Asking Price' is your target variable\n",
    "y = df['Asking Price']\n",
    "\n",
    "# Split the dataset into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "76c36c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example price thresholds\n",
    "low_price_threshold = 1300000\n",
    "high_price_threshold = 3000000\n",
    "\n",
    "# Splitting the dataset\n",
    "low_price_data = df[df['Asking Price'] <= low_price_threshold]\n",
    "mid_price_data = df[(df['Asking Price'] > low_price_threshold) & (df['Asking Price'] <= high_price_threshold)]\n",
    "high_price_data = df[df['Asking Price'] > high_price_threshold]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "e73dca4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - Low Price Segment:\n",
      "Low Price - Random Forest MSE: 17910844911.29493\n",
      "Low Price - Random Forest MAE: 71287.36641296934\n",
      "Low Price - Relative Error: 259.12877192742553%\n",
      "Low Price - Best Parameters: {'max_depth': 150, 'min_samples_split': 5, 'n_estimators': 130}\n",
      "Random Forest - Mid Price Segment:\n",
      "Mid Price - Random Forest MSE: 30468954669.377533\n",
      "Mid Price - Random Forest MAE: 116911.53721549566\n",
      "Mid Price - Relative Error: 6.04629555148975%\n",
      "Mid Price - Best Parameters: {'max_depth': 30, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "Random Forest - High Price Segment:\n",
      "High Price - Random Forest MSE: 2558143125667.311\n",
      "High Price - Random Forest MAE: 595042.6778322255\n",
      "High Price - Relative Error: 8.186610528423794%\n",
      "High Price - Best Parameters: {'max_depth': 70, 'min_samples_split': 2, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import joblib\n",
    "def train_rf_and_evaluate(data, segment_name):\n",
    "    X = data.drop(['Asking Price', 'geohash', 'Latitude', 'Longitude'], axis=1)\n",
    "    y = data['Asking Price']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    rf = RandomForestRegressor()\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 130, 150, 250, 300],\n",
    "        'max_depth': [20, 30, 70, 150],\n",
    "        'min_samples_split': [2, 5, 8]\n",
    "    }\n",
    "    grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_rf = grid_search.best_estimator_\n",
    "    \n",
    "    y_pred = best_rf.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    relative_error = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "    \n",
    "    print(f'{segment_name} - Random Forest MSE: {mse}')\n",
    "    print(f'{segment_name} - Random Forest MAE: {mae}')\n",
    "    print(f'{segment_name} - Relative Error: {relative_error}%')\n",
    "    print(f\"{segment_name} - Best Parameters:\", grid_search.best_params_)\n",
    "    \n",
    "    # Save the best model in the current working directory\n",
    "    save_path = os.getcwd()\n",
    "    save_file = os.path.join(save_path, f\"best_rf_{segment_name}.joblib\")\n",
    "    joblib.dump(best_rf, save_file)\n",
    "\n",
    "# Assuming low_price_data, mid_price_data, and high_price_data are already defined and appropriately preprocessed\n",
    "print(\"Random Forest - Low Price Segment:\")\n",
    "train_rf_and_evaluate(low_price_data, \"Low Price\")\n",
    "\n",
    "print(\"Random Forest - Mid Price Segment:\")\n",
    "train_rf_and_evaluate(mid_price_data, \"Mid Price\")\n",
    "\n",
    "print(\"Random Forest - High Price Segment:\")\n",
    "train_rf_and_evaluate(high_price_data, \"High Price\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
